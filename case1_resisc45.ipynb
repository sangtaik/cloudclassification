{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312117e9-baf8-4063-ac7d-60498481b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import_ipynb in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: nbformat in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from import_ipynb) (5.2.0)\n",
      "Requirement already satisfied: IPython in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from import_ipynb) (8.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from IPython->import_ipynb) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from IPython->import_ipynb) (3.0.20)\n",
      "Requirement already satisfied: stack-data in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from IPython->import_ipynb) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from IPython->import_ipynb) (58.0.4)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from IPython->import_ipynb) (0.18.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from IPython->import_ipynb) (5.1.1)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from IPython->import_ipynb) (5.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from IPython->import_ipynb) (0.4.4)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from IPython->import_ipynb) (2.11.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from IPython->import_ipynb) (0.1.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from IPython->import_ipynb) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import_ipynb) (0.2.5)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nbformat->import_ipynb) (4.4.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nbformat->import_ipynb) (4.9.2)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->import_ipynb) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->import_ipynb) (21.4.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jupyter-core->nbformat->import_ipynb) (302)\n",
      "Requirement already satisfied: asttokens in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from stack-data->IPython->import_ipynb) (2.0.5)\n",
      "Requirement already satisfied: executing in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from stack-data->IPython->import_ipynb) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from stack-data->IPython->import_ipynb) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from asttokens->stack-data->IPython->import_ipynb) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (4.5.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-datasets) (1.21.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-datasets) (2.27.1)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-datasets) (1.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-datasets) (4.64.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-datasets) (1.0.0)\n",
      "Requirement already satisfied: dill in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-datasets) (0.3.4)\n",
      "Requirement already satisfied: six in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-datasets) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-datasets) (3.19.4)\n",
      "Requirement already satisfied: promise in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: termcolor in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2021.10.8)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.56.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sangt\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tqdm->tensorflow-datasets) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install import_ipynb\n",
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab8a249-25e2-495e-8a6f-983e1cac4bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'accentdb',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'ag_news_subset',\n",
       " 'ai2_arc',\n",
       " 'ai2_arc_with_ir',\n",
       " 'amazon_us_reviews',\n",
       " 'anli',\n",
       " 'arc',\n",
       " 'asset',\n",
       " 'assin2',\n",
       " 'bair_robot_pushing_small',\n",
       " 'bccd',\n",
       " 'beans',\n",
       " 'bee_dataset',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'blimp',\n",
       " 'booksum',\n",
       " 'bool_q',\n",
       " 'c4',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cardiotox',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cfq',\n",
       " 'cherry_blossoms',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'citrus_leaves',\n",
       " 'cityscapes',\n",
       " 'civil_comments',\n",
       " 'clevr',\n",
       " 'clic',\n",
       " 'clinc_oos',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coco_captions',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'common_voice',\n",
       " 'coqa',\n",
       " 'cos_e',\n",
       " 'cosmos_qa',\n",
       " 'covid19',\n",
       " 'covid19sum',\n",
       " 'crema_d',\n",
       " 'cs_restaurants',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'd4rl_adroit_door',\n",
       " 'd4rl_adroit_hammer',\n",
       " 'd4rl_adroit_pen',\n",
       " 'd4rl_adroit_relocate',\n",
       " 'd4rl_antmaze',\n",
       " 'd4rl_mujoco_ant',\n",
       " 'd4rl_mujoco_halfcheetah',\n",
       " 'd4rl_mujoco_hopper',\n",
       " 'd4rl_mujoco_walker2d',\n",
       " 'dart',\n",
       " 'davis',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'dementiabank',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'diamonds',\n",
       " 'div2k',\n",
       " 'dmlab',\n",
       " 'doc_nli',\n",
       " 'dolphin_number_word',\n",
       " 'domainnet',\n",
       " 'downsampled_imagenet',\n",
       " 'drop',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'e2e_cleaned',\n",
       " 'efron_morris75',\n",
       " 'emnist',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flic',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'forest_fires',\n",
       " 'fuss',\n",
       " 'gap',\n",
       " 'geirhos_conflict_stimuli',\n",
       " 'gem',\n",
       " 'genomics_ood',\n",
       " 'german_credit_numeric',\n",
       " 'gigaword',\n",
       " 'glue',\n",
       " 'goemotions',\n",
       " 'gov_report',\n",
       " 'gpt3',\n",
       " 'gref',\n",
       " 'groove',\n",
       " 'grounded_scan',\n",
       " 'gsm8k',\n",
       " 'gtzan',\n",
       " 'gtzan_music_speech',\n",
       " 'hellaswag',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'howell',\n",
       " 'i_naturalist2017',\n",
       " 'i_naturalist2018',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet2012_multilabel',\n",
       " 'imagenet2012_real',\n",
       " 'imagenet2012_subset',\n",
       " 'imagenet_a',\n",
       " 'imagenet_lt',\n",
       " 'imagenet_r',\n",
       " 'imagenet_resized',\n",
       " 'imagenet_sketch',\n",
       " 'imagenet_v2',\n",
       " 'imagenette',\n",
       " 'imagewang',\n",
       " 'imdb_reviews',\n",
       " 'irc_disentanglement',\n",
       " 'iris',\n",
       " 'istella',\n",
       " 'kddcup99',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lambada',\n",
       " 'lfw',\n",
       " 'librispeech',\n",
       " 'librispeech_lm',\n",
       " 'libritts',\n",
       " 'ljspeech',\n",
       " 'lm1b',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'lvis',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'math_qa',\n",
       " 'mctaco',\n",
       " 'mlqa',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'movie_lens',\n",
       " 'movie_rationales',\n",
       " 'movielens',\n",
       " 'moving_mnist',\n",
       " 'mslr_web',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'natural_questions',\n",
       " 'natural_questions_open',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'nyu_depth_v2',\n",
       " 'ogbg_molpcba',\n",
       " 'omniglot',\n",
       " 'open_images_challenge2019_detection',\n",
       " 'open_images_v4',\n",
       " 'openbookqa',\n",
       " 'opinion_abstracts',\n",
       " 'opinosis',\n",
       " 'opus',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'pass',\n",
       " 'patch_camelyon',\n",
       " 'paws_wiki',\n",
       " 'paws_x_wiki',\n",
       " 'penguins',\n",
       " 'pet_finder',\n",
       " 'pg19',\n",
       " 'piqa',\n",
       " 'places365_small',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'protein_net',\n",
       " 'qa4mre',\n",
       " 'qasc',\n",
       " 'quac',\n",
       " 'quality',\n",
       " 'quickdraw_bitmap',\n",
       " 'race',\n",
       " 'radon',\n",
       " 'reddit',\n",
       " 'reddit_disentanglement',\n",
       " 'reddit_tifu',\n",
       " 'ref_coco',\n",
       " 'resisc45',\n",
       " 'rlu_atari',\n",
       " 'rlu_atari_checkpoints',\n",
       " 'rlu_atari_checkpoints_ordered',\n",
       " 'rlu_dmlab_explore_object_rewards_few',\n",
       " 'rlu_dmlab_explore_object_rewards_many',\n",
       " 'rlu_dmlab_rooms_select_nonmatching_object',\n",
       " 'rlu_dmlab_rooms_watermaze',\n",
       " 'rlu_dmlab_seekavoid_arena01',\n",
       " 'rlu_rwrl',\n",
       " 'robomimic_ph',\n",
       " 'robonet',\n",
       " 'robosuite_panda_pick_place_can',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 's3o4d',\n",
       " 'salient_span_wikipedia',\n",
       " 'samsum',\n",
       " 'savee',\n",
       " 'scan',\n",
       " 'scene_parse150',\n",
       " 'schema_guided_dialogue',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'scrolls',\n",
       " 'sentiment140',\n",
       " 'shapes3d',\n",
       " 'siscore',\n",
       " 'smallnorb',\n",
       " 'smartwatch_gestures',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'speech_commands',\n",
       " 'spoken_digit',\n",
       " 'squad',\n",
       " 'squad_question_generation',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'star_cfq',\n",
       " 'starcraft_video',\n",
       " 'stl10',\n",
       " 'story_cloze',\n",
       " 'summscreen',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'symmetric_solids',\n",
       " 'tao',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tedlium',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'tiny_shakespeare',\n",
       " 'titanic',\n",
       " 'trec',\n",
       " 'trivia_qa',\n",
       " 'tydi_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'vctk',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'voxceleb',\n",
       " 'voxforge',\n",
       " 'waymo_open_dataset',\n",
       " 'web_nlg',\n",
       " 'web_questions',\n",
       " 'wider_face',\n",
       " 'wiki40b',\n",
       " 'wiki_auto',\n",
       " 'wiki_bio',\n",
       " 'wiki_table_questions',\n",
       " 'wiki_table_text',\n",
       " 'wikiann',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wikipedia_toxicity_subtypes',\n",
       " 'wine_quality',\n",
       " 'winogrande',\n",
       " 'wit',\n",
       " 'wit_kaggle',\n",
       " 'wmt13_translate',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'wordnet',\n",
       " 'wsc273',\n",
       " 'xnli',\n",
       " 'xquad',\n",
       " 'xsum',\n",
       " 'xtreme_pawsx',\n",
       " 'xtreme_xnli',\n",
       " 'yelp_polarity_reviews',\n",
       " 'yes_no']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # .py로 매번 수정하기 귀찮아서 ipynb 파일로 import가능하게 만드는 라이브러리\n",
    "import import_ipynb \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import models\n",
    "\n",
    "\n",
    "GoogLeNet = models.InceptionNet\n",
    "\n",
    "tfds.list_builders()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a3b5343-c0bb-440c-844b-b65c3f8cd986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\sangt\\tensorflow_datasets\\resisc45\\3.0.0...\u001b[0m\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Manual directory C:\\Users\\sangt\\tensorflow_datasets\\downloads\\manual does not exist or is empty. Create it and download/extract dataset artifacts in there using instructions:\nDataset can be downloaded from OneDrive:\nhttps://1drv.ms/u/s!AmgKYzARBl5ca3HNaHIlzp_IXjs\nAfter downloading the rar file, please extract it to the manual_dir.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 데이터 생성\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m dataset, info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresisc45\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m dataset_test, dataset_train \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:325\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m    324\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 325\u001b[0m   dbuilder\u001b[38;5;241m.\u001b[39mdownload_and_prepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:462\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m   \u001b[38;5;66;03m# Old version of TF are not os.PathLike compatible\u001b[39;00m\n\u001b[0;32m    461\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf_compat\u001b[38;5;241m.\u001b[39mmock_gfile_pathlike():\n\u001b[1;32m--> 462\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    468\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    469\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    470\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    471\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1157\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1156\u001b[0m   optional_pipeline_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1157\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(  \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m     dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptional_pipeline_kwargs)\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[0;32m   1163\u001b[0m     split_generators\u001b[38;5;241m=\u001b[39msplit_generators,\n\u001b[0;32m   1164\u001b[0m     generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples,\n\u001b[0;32m   1165\u001b[0m     is_beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[0;32m   1166\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_datasets\\image_classification\\resisc45.py:88\u001b[0m, in \u001b[0;36mResisc45._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split_generators\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[0;32m     87\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns SplitGenerators.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m   path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_dir\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNWPU-RESISC45\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     89\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must download the dataset manually from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextract it, and place it in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     92\u001b[0m                              _URL, dl_manager\u001b[38;5;241m.\u001b[39mmanual_dir))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_datasets\\core\\utils\\py_utils.py:152\u001b[0m, in \u001b[0;36mmemoized_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m    150\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m   cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    153\u001b[0m   \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:649\u001b[0m, in \u001b[0;36mDownloadManager.manual_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    646\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo access `dl_manager.manual_dir`, please set \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    647\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`MANUAL_DOWNLOAD_INSTRUCTIONS` in your dataset.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manual_dir\u001b[38;5;241m.\u001b[39mexists() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manual_dir\u001b[38;5;241m.\u001b[39miterdir()):\n\u001b[1;32m--> 649\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    650\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mManual directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manual_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is empty. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    651\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreate it and download/extract dataset artifacts in there using \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    652\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manual_dir_instructions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manual_dir\n",
      "\u001b[1;31mAssertionError\u001b[0m: Manual directory C:\\Users\\sangt\\tensorflow_datasets\\downloads\\manual does not exist or is empty. Create it and download/extract dataset artifacts in there using instructions:\nDataset can be downloaded from OneDrive:\nhttps://1drv.ms/u/s!AmgKYzARBl5ca3HNaHIlzp_IXjs\nAfter downloading the rar file, please extract it to the manual_dir."
     ]
    }
   ],
   "source": [
    "#하이퍼 파라미터\n",
    "batch_size = 128\n",
    "num_epoch = 100\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "def normalize(dataset):    \n",
    "    image, label = tf.cast(dataset['image'], tf.float32) / 255.0, dataset['label']\n",
    "    return image, label\n",
    "\n",
    "# 데이터 생성\n",
    "dataset, info = tfds.load('resisc45', as_supervised = True, with_info = True)\n",
    "dataset_test, dataset_train = dataset['test'], dataset['train']\n",
    "\n",
    "# open_images_dataset = tfds.image.OpenImagesV4()\n",
    "# open_images_dataset.download_and_prepare(download_dir=\"/notebooks/dataset/\")\n",
    "#ds = tf.load('open_images_v4', data_dir='/notebooks/open_images_dataset/extracted', download=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77325c5-1990-44ed-b0a1-ee25e5ae38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_train = dataset_train.map(normalize).shuffle(10000).batch(batch_size)\n",
    "dataset_test = dataset_test.map(normalize).batch(batch_size)\n",
    "\n",
    "datagen = ImageDataGenerator(rotation_range=10, horizontal_flip=True, zoom_range=0.1)\n",
    "\n",
    "# GoogleNet 객체 생성\n",
    "model = GoogLeNet((256, 256, 3), 10)\n",
    "model.build(input_shape = (None, 256, 256, 3))\n",
    "model.summary()\n",
    "\n",
    "# Prepare Taining 라벨링\n",
    "\n",
    "############## #사용자 루프 모델 학습 ######\n",
    "# model.compile() 기능을 사용자가 직접 수동으로 하는 것\n",
    "#def compile_by_users():\n",
    "    \n",
    "for image, label in dataset_train.take(1):\n",
    "    fig, axes = plt.subplots(8, 4)\n",
    "    fig.set_size_inches(10, 16)\n",
    "    for i in range(32):\n",
    "        axes[i//4, i%4].imshow(image[i])\n",
    "        axes[i//4, i%4].axis('off')\n",
    "        axes[i//4, i%4].set_title(label_map[label[i].numpy()], fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda72e01-159e-4db0-904e-474d44933ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값에 대해 원핫라벨링한 뒤, 교차 엔트로피에 통과하여 loss를 검사\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy()\n",
    "# 최적화는 adam\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "train_loss = keras.metrics.Mean(name = 'train_loss')\n",
    "train_accuracy = keras.metrics.SparseCategoricalAccuracy(name = 'train_accuracy')\n",
    "test_loss = keras.metrics.Mean(name = 'test_loss')\n",
    "test_accuracy = keras.metrics.SparseCategoricalAccuracy(name = 'test_accuracy')\n",
    "    \n",
    "\n",
    "\n",
    "#compile_by_users()\n",
    "\n",
    "def train_step(image, label): # 훈련 이미지, 훈련 레이블\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        predictions = model(image)\n",
    "        loss = loss_object(label, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(label, predictions)\n",
    "    \n",
    "def test_step(image, label): # 훈련 이미지, 훈련 레이블\n",
    "        \n",
    "    predictions = model(image)\n",
    "    loss = loss_object(label, predictions)\n",
    "    \n",
    "    test_loss(loss)\n",
    "    test_accuracy(label, predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358724a-49b6-4184-967d-e7382d52f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa904c2-f18b-4c0a-bce7-a0fccdeebf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    i = 0\n",
    "    for image, label in dataset_train:\n",
    "        for _image, _label in datagen.flow(image, label, batch_size = batch_size):\n",
    "            printProgressBar(i + 1, len(dataset_train),  prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "            # Data Augmentabtion된 훈련 데이터를 _image, 훈련 레이블을 _label로 저장\n",
    "            # Data Augmentabtion이란 데이터셋을 여러 방법으로 변경하여 데이터셋 개수를 늘리는 방법이다.\n",
    "            # 회전을 시킨다거나, 임의의 범위를 잘라낸다건, 하나의 소르를 가지고 데이터를 우려내는 방법이다.\n",
    "            train_step(_image, _label) # 모델에 data Augmentation된 훈련 데이터셋을 넣음\n",
    "            i+=1\n",
    "            break\n",
    "    # 훈련 데이터 셋에 대한 학습 종료\n",
    "\n",
    "    for test_image, test_label in dataset_test:\n",
    "        test_step(test_image, test_label)\n",
    "        # 시험 데이터 셋 테스트 종료\n",
    "        \n",
    "    train_accuracies.append(train_accuracy.result())\n",
    "    test_accuracies.append(test_accuracy.result())\n",
    "    \n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch + 1, train_loss.result(), train_accuracy.result() * 100,\n",
    "                          test_loss.result(), test_accuracy.result() * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
